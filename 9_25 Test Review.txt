Preprocessing
	Common steps
		Figure out the dimensions and shape of the problem (m_train, m_test, num_px, ...)
		Reshape the datasets such that each example is now a vector of size (num_px * num_px * 3)
		Standardize the data

y = cat (1) or non-cat(0)
training set of m_train images
test set of m_test images
each image of shape (num_px, num_px, 3) 3 because RGB. Each color is represented by num_px x num_px values. Makes sense
	m_train = 209
	m_test = 50
	num_px = 64
	
Reshape
	train_set_x_flatten is 12288 x 209 (64 * 64 * 3, m_train)
Standardize dataset
	Common preprocessing step in machine learning. Subtract the mean of thee whole numpy array from each example, then divide each example by standard deviation of the whole numpy array
		But for picture datasets, it's much simpler and works almost as well to divide every row by 255.
		
	train_set_x = train_set_x_flatten/255
	test_set_x = test_set_x_flatten/255
	
z(i) = w.T * x(i) + b
yHat(i) = a(i) = sigmoid(z(i)) = prediction
	a(i) is either 0 or 1. 1 if we predict a cat picture.
Loss = L(a(i), y(i)) = -y(i) * log(a(i)) - (1 - y(i) * log(1 - a(i)) 
	So if y is a cat, Loss is log(a(i)). If y is not a cat, Loss is log(1 - a(i)).
	
Cost is then computed by summing over all training examples
	J = 1/m * sum(Loss)
	
Main steps for building a NN are
	1. Define the model structure (such as number of input features)
	2. Initialize the model's parameters
	3 Loop
		-Calculate current loss (forward propagation)
		-Calculate current gradient (backward propagation)
		-Update parameters (gradient descent)
		
	Initalize parameters - Initialize with zeros.
		Initializes vector, w, of shape num_px * num_px * 3, 1, with zeros
		Initalizes b with a zero
		
	Calculate loss and gradient (forward and backward propagation)
		m = X.shape[1] // number of images.
		A = sigmoid(np.dot(w.T, X) + b) // our prediction. A = {0, 1}
		cost = (-1 / m) * (np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A) //note the negative was moved to (-1/m)
		
		Gradients - Backward propagation
		you have dw and db because w and b need to be updated.
		dw = (1 / m) * X(A - Y).T
		db = (1 / m) * np.sum(A - Y)
		
	Update parameters
		Update rule is x = x - alpha(dx) // alpha is the learning rate
			//higher learning rate means quicker learning, but might overstep the optimal answer
			
Cost for this program went from .7 to .14. With Cost being the mean of Loss for all images. Remember that y and yHat are between 1 and 0, so .7 is very high!